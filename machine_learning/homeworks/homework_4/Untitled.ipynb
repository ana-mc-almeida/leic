{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e82e1ee-7b6a-4f09-9e7e-b2df08030e9e",
   "metadata": {},
   "source": [
    "# II. Programming and critical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95f00c-cdff-4d8b-8caa-c2d2e2a7a833",
   "metadata": {},
   "source": [
    "Recall the column_diagnosis.arff dataset from previous homeworks. For the following exercises,\n",
    "normalize the data using sklearn‚Äôs MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed00c0f-a394-4630-b631-a3ea1a3ba6d6",
   "metadata": {},
   "source": [
    "##### Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc997e0-9826-44db-9a3a-d52610b6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Reading the ARFF file\n",
    "data = loadarff('column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "true_labels = df['class'].str.decode('utf-8')\n",
    "#print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6354c319-cca2-4f31-81f4-12a8b3ace536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Hernia', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Spondylolisthesis', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "true_labels = df['class'].tolist()\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9917a0e5-3204-4fa3-a70e-b7583ae4d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ecbf17-8cce-4e6a-bb23-a641d7db3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'class' column\n",
    "df['class_encoded'] = label_encoder.fit_transform(df['class'])\n",
    "\n",
    "# Drop the original 'class' column if you don't need it\n",
    "df = df.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93a10de-ab7d-46eb-b5bf-63ae9a787fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = df.drop(columns=['class'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da365795-22c5-412e-a4d0-7e4af3e15cf9",
   "metadata": {},
   "source": [
    "## 1)\n",
    "Using sklearn, apply k-means clustering fully unsupervisedly on the normalized data with ùëò ‚àà {2,3,4,5} (random=0 and remaining parameters as default). Assess the silhouette and purity of the produced solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6728e9-a7b3-4de8-80fc-1e6a728b620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/ana/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/ana/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=2: \n",
      "\tSilhouette Score: 0.36044124340441114\n",
      "\tPurity: 0.19421751845896906\n",
      "\n",
      "\n",
      "K=3: \n",
      "\tSilhouette Score: 0.29579055730002257\n",
      "\tPurity: 0.27137462044401917\n",
      "\n",
      "\n",
      "K=4: \n",
      "\tSilhouette Score: 0.27442402122340176\n",
      "\tPurity: 0.2727230243601453\n",
      "\n",
      "\n",
      "K=5: \n",
      "\tSilhouette Score: 0.23823928397844843\n",
      "\tPurity: 0.3372385188316694\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "\n",
    "# Initialize a list to store silhouette and purity scores for different k values\n",
    "silhouette_scores = []\n",
    "purity_scores = []\n",
    "\n",
    "# Define a range of k values\n",
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "# Perform k-means clustering for different k values\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(normalized_data)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(normalized_data, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Calculate purity\n",
    "    # Assuming you have ground truth labels in a variable named 'true_labels'\n",
    "    purity = homogeneity_score(true_labels, cluster_labels)\n",
    "    purity_scores.append(purity)\n",
    "\n",
    "# Print silhouette and purity scores for different k values\n",
    "for k, silhouette, purity in zip(k_values, silhouette_scores, purity_scores):\n",
    "    print(f'K={k}: \\n\\tSilhouette Score: {silhouette}\\n\\tPurity: {purity}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a463d2-0cee-445a-ac92-633aada4efa7",
   "metadata": {},
   "source": [
    "## 2)\n",
    "Consider the application of PCA after the data normalization:  \n",
    "    i. Identify the variability explained by the top two principal components.  \n",
    "    ii. For each one of these two components, sort the input variables by relevance by inspecting the absolute weights of the linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cdb5d7-408d-44a7-900c-825e264983d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variability explained by the top two principal components: 77.14%\n",
      "Top variables for PC1 (by absolute weight): Variable 0, Variable 2, Variable 1, Variable 3, Variable 5, Variable 4\n",
      "Top variables for PC2 (by absolute weight): Variable 1, Variable 4, Variable 3, Variable 0, Variable 2, Variable 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(normalized_data)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "variance_explained_by_top_two = np.sum(explained_variance_ratio[:2])\n",
    "print(f\"Variability explained by the top two principal components: {variance_explained_by_top_two:.2%}\")\n",
    "\n",
    "# Step 4: Sort the input variables by relevance based on absolute weights of the linear projection\n",
    "# Get the loadings (weights) of the top two principal components\n",
    "top_two_loadings = pca.components_[:2]\n",
    "\n",
    "# Create a dictionary to store the absolute loadings and their corresponding variable names\n",
    "loadings_dict = {f'PC{i+1}': abs(top_two_loadings[i]) for i in range(2)}\n",
    "#variable_names = [f'{label}' for label in true_labels]\n",
    "variable_names = [f'Variable {i}' for i in range(df.shape[1])]\n",
    "\n",
    "\n",
    "# Sort variables by relevance for each principal component\n",
    "sorted_variables = {\n",
    "    f'PC{i+1}': [variable_names[j] for j in np.argsort(loadings_dict[f'PC{i+1}'])][::-1]\n",
    "    for i in range(2)\n",
    "}\n",
    "\n",
    "# Print the sorted variables by relevance\n",
    "for i in range(2):\n",
    "    print(f\"Top variables for PC{i+1} (by absolute weight): {', '.join(sorted_variables[f'PC{i+1}'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26f79e6-c204-4b5b-adda-f0dcd76e8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance of PC1: 0.5618144484299207\n",
      "Explained Variance of PC2: 0.20955952591361918\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(normalized_data)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(f\"Explained Variance of PC1: {explained_variance[0]}\")\n",
    "print(f\"Explained Variance of PC2: {explained_variance[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a33957-b466-4e9d-9ed9-1d67f25cce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance by the Top Two Principal Components: 0.77137397434354\n",
      "Variables sorted by relevance for the first principal component:\n",
      "['Variable 0', 'Variable 2', 'Variable 1', 'Variable 3', 'Variable 5', 'Variable 4']\n",
      "Variables sorted by relevance for the second principal component:\n",
      "['Variable 1', 'Variable 4', 'Variable 3', 'Variable 0', 'Variable 2', 'Variable 5']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'X' is your normalized data\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(normalized_data, rowvar=False)\n",
    "\n",
    "# Perform PCA and obtain eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "\n",
    "# Calculate the explained variance for the top two components\n",
    "explained_variance_ratio = (sorted_eigenvalues[0] + sorted_eigenvalues[1]) / np.sum(sorted_eigenvalues)\n",
    "\n",
    "print(\"Explained Variance by the Top Two Principal Components:\", explained_variance_ratio)\n",
    "\n",
    "# Assuming you have already performed PCA and have eigenvectors in 'eigenvectors'\n",
    "# Select the top two principal components\n",
    "top_two_components = eigenvectors[:, :2]\n",
    "\n",
    "# Sort input variables by relevance to the first principal component\n",
    "sorted_indices_pc1 = np.argsort(np.abs(top_two_components[:, 0]))[::-1]\n",
    "variables_sorted_by_relevance_pc1 = sorted_indices_pc1  # This will give you the indices of variables\n",
    "\n",
    "# Sort input variables by relevance to the second principal component\n",
    "sorted_indices_pc2 = np.argsort(np.abs(top_two_components[:, 1]))[::-1]\n",
    "variables_sorted_by_relevance_pc2 = sorted_indices_pc2  # This will give you the indices of variables\n",
    "\n",
    "# If you want the actual variable names, assuming 'X' is your data\n",
    "variable_names = [f'Variable {i}' for i in range(df.shape[1])]\n",
    "\n",
    "# Get the variable names sorted by relevance for the first principal component\n",
    "variables_sorted_names_pc1 = [variable_names[i] for i in variables_sorted_by_relevance_pc1]\n",
    "\n",
    "# Get the variable names sorted by relevance for the second principal component\n",
    "variables_sorted_names_pc2 = [variable_names[i] for i in variables_sorted_by_relevance_pc2]\n",
    "\n",
    "print(\"Variables sorted by relevance for the first principal component:\")\n",
    "print(variables_sorted_names_pc1)\n",
    "\n",
    "print(\"Variables sorted by relevance for the second principal component:\")\n",
    "print(variables_sorted_names_pc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d784a9-0dc5-4195-ba6d-af579b645b14",
   "metadata": {},
   "source": [
    "## 3)\n",
    "Visualize side-by-side the data using: i) the ground diagnoses, and ii) the previously learned ùëò = 3 clustering solution. To this end, projected the normalized data onto a 2-dimensional data space using PCA and then color observations using the reference and cluster annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094d491c-fcae-4fc9-aa15-f64a4c359b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(normalized_data)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Perform k-means clustering\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m kmeans_labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(normalized_data)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create a scatter plot for the ground diagnoses\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(normalized_data)\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(normalized_data)\n",
    "\n",
    "# Create a scatter plot for the ground diagnoses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.title(\"Ground Diagnoses\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "# Assuming you have the clustering results in the variable 'kmeans_labels'\n",
    "# Create a scatter plot for the clustering solution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis')\n",
    "plt.title(\"K-Means Clustering (k=3)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3275add-7016-468f-9129-e6ee204002e8",
   "metadata": {},
   "source": [
    "## 4)\n",
    "Considering the results from questions (1) and (3), identify two ways on how clustering can\n",
    "be used to characterize the population of ill and healthy individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda5df6-91ac-41b0-b772-edae6edf6e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
